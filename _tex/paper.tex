% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{agujournal2019}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{url} %this package should fix any errors with URLs in refs.
\usepackage{lineno}
\usepackage[inline]{trackchanges} %for better track changes. finalnew option will compile document with changes incorporated.
\usepackage{soul}
\linenumbers
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Evaluating Cloud-Optimized HDF5 for NASA's ICESat-2 Mission},
  pdfauthor={Luis A. Lopez; Andrew P. Barrett; Amy Steiker; Aleksandar Jelenak; Lisa Kaser; Jeffrey E. Lee},
  pdfkeywords={cloud-native, cloud, HDF5, NASA, ICESat-2},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\journalname{Geophysical Research Letters}

\draftfalse

\begin{document}
\title{Evaluating Cloud-Optimized HDF5 for NASA's ICESat-2 Mission}

\authors{Luis A. Lopez\affil{1}, Andrew P. Barrett\affil{1}, Amy
Steiker\affil{1}, Aleksandar Jelenak\affil{2}, Lisa
Kaser\affil{1}, Jeffrey E. Lee\affil{3}}
\affiliation{1}{CIRES, National Snow and Ice Data Center, University of
Colorado, Boulder., Boulder, CO, USA}\affiliation{2}{The HDF
Group, Champaign, IL, USA}\affiliation{3}{NASA Goddard Space Flight
Center, NASA / KBR, Greenbelt, MD, USA}



\begin{abstract}
The Hierarchical Data Format (HDF) is a common archival format for
n-dimensional scientific data; it has been utilized to store valuable
information from astrophysics to earth sciences and everything in
between. As flexible and powerful as HDF can be, it comes with big
tradeoffs when it's accessed from remote storage systems, mainly because
the file format and the client I/O libraries were designed for local and
supercomputing workflows. As scientific data and workflows migrate to
the cloud , efficient access to data stored in HDF format is a key
factor that will accelerate or slow down ``science in the cloud'' across
all disciplines. We present an implementation of recently available
features in the HDF5 stack that results in performant access to HDF from
remote cloud storage. This performance is on par with modern
cloud-native formats like Zarr but with the advantage of not having to
reformat data or generate metadata sidecar files (DMR++, Kerchunk). Our
benchmarks also show potential cost-savings for data producers if their
data are processed using cloud-optimized strategies.
\end{abstract}





\section{Problem}\label{problem}

Scientific data from NASA and other agencies are increasingly being
distributed from the commercial cloud. Cloud storage enables large-scale
workflows and should reduce local storage costs. It also allows the use
of scalable on-demand cloud computing resources by individual scientists
and the broader scientific community. However, the majority of this
scientific data is stored in a format that was not designed for the
cloud: The Hierarchical Data format or HDF.

The most recent version of the Hierarchical Data Format is HDF5, a
common archival format for n-dimensional scientific data; it has been
utilized to store valuable information from astrophysics to earth
sciences and everything in between. As flexible and powerful as HDF5 can
be, it comes with big trade-offs when it's accessed from remote storage
systems.

HDF5 is a complex file format; we can think of it as a file system using
a tree-like structure with multiple data types and native data
structures. Because of this complexity, the most reliable way of
accessing data stored in this format is using the HDF5 C API. Regardless
of access pattern, nearly all tools ultimately rely on the HDF5-C
library and this brings a couple issues that affect the efficiency of
accessing this format over the network:

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{\texorpdfstring{\textbf{Metadata
fragmentation}}{Metadata fragmentation}}\label{metadata-fragmentation}

When working with large datasets, especially those that include numerous
variables and nested groups, the storage of file-level metadata can
become a challenge. By default, metadata associated with each dataset is
stored in chunks of 4 kilobytes (KB). This chunking mechanism was
originally intended to optimize storage efficiency and access speed on
disks with hardware resources available more than 20 years ago. In
datasets with many variables and/or complex hierarchical structures,
these 4KB chunks can lead to significant fragmentation.

Fragmentation occurs when this metadata is spread out across multiple
non-contiguous chunks within the file. This results in inefficiencies
when accessing or modifying data because compatible libraries need to
read from multiple, scattered locations in the file. Over time, as the
dataset grows and evolves, this fragmentation can compound, leading to
degraded performance and increased storage overhead. In particular,
operations that involve reading or writing metadata, such as opening a
file, checking attributes, or modifying variables, can become slower and
more resource-intensive.

\subsubsection{\texorpdfstring{\textbf{Global API
Lock}}{Global API Lock}}\label{global-api-lock}

Because of the historical complexity of operations with the HDF5
format(The HDF Group, n.d.), there has been a necessity to make the
library thread-safe and similarly to what happens in the Python
language, the simplest mechanism to implement this is to have a global
API lock. This global lock is not as big of an issue when we read data
from local disk but it becomes a major bottleneck when we read data over
the network because each read is sequential and latency in the cloud is
exponentially bigger than local access (MDN, 2024) (Scott, 2020).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure*}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{figures/figure-1.png}}

}

\caption{\label{fig-1}shows how reads (Rn) are done in order to access
file metadata, In the first read, R0, the HDF5 library verifies the file
signature from the superblock, subsequent reads, R1, R2,\ldots Rn, read
file metadata, 4kb at the time.}

\end{figure*}%

\subsubsection{\texorpdfstring{\textbf{Background and data
selection}}{Background and data selection}}\label{background-and-data-selection}

As a result of community feedback and ``hack weeks'' organized by NSIDC
and UW eScience Institute in 2023(ICESAT-2 HackWeek, 2023), NSIDC
started the Cloud Optimized Format Investigation (COFI) project to
improve access to HDF5 from the ICESat-2 mission, a spaceborne lidar
that retrieves surface topography of the Earth's ice sheets, land and
oceans (Neumann et al., 2019). Because of its complexity, large size and
importance for cryospheric studies we targeted the ATL03 data product.
The most relevant variable in ATL03 are geolocated photon heights from
the ICESat-2 ATLAS instrument. Each ATL03 file contains 1003 geophysical
variables in 6 data groups. Although our research was focused on this
dataset, most of our findings are applicable to any dataset stored in
HDF5 and NetCDF4.

\section{Methodology}\label{methodology}

We tested access times to original and different configurations of
cloud-optimized HDF5
\href{https://its-live-data.s3.amazonaws.com/index.html\#test-space/cloud-experiments/h5cloud/}{ATL03
files} stored in AWS S3 buckets in region us-west-2, the region hosting
NASA's Earthdata Cloud archives. Files were accessed using Python tools
commonly used by Earth scientists: h5py and Xarray(Hoyer \& Hamman,
2017). h5py is a Python wrapper around the HDF5 C API. xarray\footnote{\texttt{h5py}
  is a dependency of Xarray} is a widely used Python package for working
with n-dimensional data. We also tested access times using h5coro, a
python package optimized for reading HDF5 files from S3 buckets and
kerchunk, a tool that creates an efficient lookup table for file chunks
to allow performant partial reads of files.

The test files were originally cloud optimized by ``repacking'' them,
using a relatively new feature in the HDF5 C API called ``paged
aggregation''. Page aggregation does 2 things: first, it collects
file-level metadata from datasets and stores it on dedicated metadata
blocks at the front of the file; second, it forces the library to write
both data and metadata using these fixed-size pages. Aggregation allows
client libraries to read file metadata with only a few requests using
the page size as a fixed request size, overriding the 1 request per
chunk behavior.

\begin{figure*}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{figures/figure-2.png}}

}

\caption{\label{fig-2}shows how file-level metadata and data gets
internally packed once we use paged aggregation on a file.}

\end{figure*}%

As we can see in Figure~\ref{fig-2}, when we cloud optimize a file using
paged-aggregation there are some considerations and behavior that we had
to take into account. The first thing to observe is that page
aggregation will -- as we mentioned -- consolidate the file-level
metadata at the front of the file and will add information in the
so-called superblock\footnote{The HDF5 superblock is a crucial component
  of the HDF5 file format, acting as the starting point for accessing
  all data within the file. It stores important metadata such as the
  version of the file format, pointers to the root group, and addresses
  for locating different file components} The next thing to notice is
page size is used across the board for metadata and data as of October
2024 and version 1.14 of the HDF5 library, page size cannot dynamically
adjust to the total metadata size.

\begin{figure*}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{figures/figure-3.png}}

}

\caption{\label{fig-3}shows how file-level metadata and data packing
inside aggregated pages leave unused space that can potentially increase
the file size in a considerable way.}

\end{figure*}%

This one page size for all approach simplifies how the HDF5 API reads
the file (if configured) but it also brings unused page space and chunk
over-reads. In the case of the ICESat-2 dataset (ATL03) the data itself
has been partitioned and each granule represents a segment in the
satellite orbit and within the file the most relevant dataset is chunked
using 10,000 items per chunk, with data being float-32 and using a fast
compression value, the resulting chunk size is on average under 40KB,
which is really small for an HTTP request, especially when we have to
read them sequentially. Because of these considerations, we opted for
testing different page sizes, and increase chunk size. The following
table describes the different configurations used in our tests.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1589}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.3444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1457}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0993}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0728}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0728}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1060}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
prefix
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\% file size increase
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textasciitilde km per chunk
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
shape
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
page size
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
avg\_chunk\_size
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
original & original file from ATL03 v6 (1gb and 7gb) & 0 & 1.5km &
(10000,) & N/A & 35kb \\
original-kerchunk & kerchunk sidecar of the original file & N/A & 1.5km
& (10000,) & N/A & 35kb \\
page-only-4mb & paged-aggregated file with 4mb per page &
\textasciitilde1\% & 1.5km & (10000,) & 4MB & 35kb \\
page-only-8mb & paged-aggregated file with 4mb per pag8 &
\textasciitilde1\% & 1.5km & (10000,) & 8MB & 35kb \\
rechunked-4mb & page-aggregated and bigger chunk sizes &
\textasciitilde1\% & 10km & (100000,) & 4MB & 400kb \\
rechunked-8mb & page-aggregated and bigger chunk sizes &
\textasciitilde1\% & 10km & (100000,) & 8MB & 400kb \\
rechunked-8mb-kerchunk & kerchunk sidecar of the last paged-aggregated
file & N/A & 10km & (100000,) & 8MB & 400kb \\
\end{longtable}

This table represents the different configurations we used for our tests
in 2 file sizes. It's worth noticing we encountered a few outlier cases
where compression and chunk sizes led page aggregation to an increase in
file size of approximately 10\% which was above the desired value for
NSIDC (5\% max). We tested these files using the most common libraries
to handle HDF5 and 2 different I/O drivers that support remote access to
AWS S3, fsspec and the native S3. The results of our testing are
explained in the next section and the code to reproduce the results is
in the attached notebooks.

\section{Results}\label{results}

\begin{figure*}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{figures/figure-4.png}}

}

\caption{\label{fig-4}Using paged aggregation alone is not a complete
solution. This behavior is caused by over-reads of data now distributed
in pages and the internals of HDF5 not knowing how to optimize the
requests. This means that if we cloud optimize alone and use the same
code, in some cases we'll make access to these files even slower. A very
important thing to notice here is that rechunking the file, in this case
using 10X bigger chunks results in a predictable 10X improvement in
access times without any cloud optimization involved. Having less chunks
generates less metadata and bigger requests, in general is it
recommended that chunk sizes should range between 1MB and 10MB{[}Add
citation, S3 and HDF5{]} and if we have enough memory and bandwidth even
bigger (Pangeo recommends up to 100MB chunks){[}Add citation.{]}}

\end{figure*}%

\begin{figure*}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{figures/figure-5.png}}

}

\caption{\label{fig-5}Once the I/O configuration is aligned with the
chunking in the file, access times perform on par with cloud optimized
access patterns like Kerchunk/Zarr. These numbers are from in-region
execution. Out of region is considerably slower for the
non-cloud-optimized case.}

\end{figure*}%

\section{Recommendations}\label{recommendations}

Based on the benckmarks we got from our tests, we have split our
recommendations for the ATL03 product into 3 main categories: creating
the files, accessing the files, and future tool development. These
recommendations aim to streamline HDF5 workflows in cloud environments,
enhancing performance and reducing costs.

\subsection{Recommended cloud
optimizations}\label{recommended-cloud-optimizations}

Based on our testing we recommend the following cloud optimizations for
creating HDF5 files for the ATL03 product:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create HDF5 files using paged aggregation by setting HDF5 library
  parameters:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    File page strategy: H5F\_FSPACE\_STRATEGY\_PAGE
  \item
    File page size: 8000000 If repacking an existing file, h5repack
    contains the code to alter these variables inside the file
  \end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
 \ExtensionTok{h5repack} \AttributeTok{{-}S}\NormalTok{ PAGE }\AttributeTok{{-}G}\NormalTok{ 8000000 input.h5 output.h5}
\end{Highlighting}
\end{Shaded}
\item
  Avoid using unlimited dimensions when creating variables because the
  HDF5 API cannot support it inside buffered pages and representation of
  these variables is not supported by Kerchunk.
\end{enumerate}

\subsubsection{Reasoning}\label{reasoning}

Based on the variable size of ATL03 it becomes really difficult to
allocate a fixed metadata page. Big files contain north of 30MB of
metadata, but the median metadata size per file is below 8MB. If we had
adopted user block we would have caused an increase in the file size and
storage cost of approximate 30\% (reference to our tests). Another
consequence of using a dedicated fixed page for file-level metadata is
that metadata overflow will generate the same impact in access times,
the library will fetch the metadata in one go but the rest will be using
the predefined block size of 4kb.

Paged aggregation is thus the simplest way of cloud optimizing an HDF5
file as the metadata will keep filling dedicated pages until all the
file-level metadata is stored at the front of the file. Chunk sizes
cannot be larger than the page size and when chunk sizes are smaller we
need to take into account how these chunks will fit on a page, in an
ideal scenario all the space will be filled but that is not the case and
we will end up with unused space See~\ref{fig-2}.

\subsection{Recommended Access
Patterns}\label{recommended-access-patterns}

As we saw in our benchmarks, efficient access to cloud optimized HDF5
files in cloud storage requires that we also optimize our access
patterns. The following recommendations focus on optimizing workflows
for Python users. However, these recommendations should be applicable
across programming languages. It's also worth mentioning that the HDF
Group aims to include some of these features in their roadmap.

\begin{itemize}
\tightlist
\item
  \textbf{Efficient Reads}: Efficiently reading cloud-hosted HDF5 files
  involves minimizing network requests and prioritizing large sequential
  reads. Configure chunk sizes between 1--10 MB to match the block sizes
  used in cloud object storage systems, ensuring meaningful data
  retrieval in each read. Avoid small chunks, as they cause excessive
  HTTP overhead and slower access speeds.
\item
  \textbf{Parallel Access}: Use parallel computing frameworks like
  \href{https://www.dask.org/}{\texttt{Dask}} or multiprocessing to
  divide read operations across multiple processes or nodes. This
  alleviates the sequential access bottleneck caused by the HDF5 global
  lock, particularly in workflows accessing multiple datasets.
\item
  \textbf{Cache Management}: Implement caching for metadata to avoid
  repetitive fetches. Tools like \texttt{fsspec} or \texttt{h5coro}
  allow in-memory or on-disk caching for frequently accessed data,
  reducing latency during high-frequency
\item
  \textbf{Regional Access}: Operate workflows in the same cloud region
  as the data to minimize costs and latency. Cross-region data transfer
  is expensive and introduces significant delays. Where possible, deploy
  virtual machines close to the data storage region.
\end{itemize}

\subsection{Recommended Tooling
Development}\label{recommended-tooling-development}

To enable widespread and efficient use of HDF5 files in cloud
environments, it is crucial to develop robust tools across all major
programming languages. The HDF Group has expressed intentions to include
these features in their roadmap, ensuring seamless compatibility with
emerging cloud storage and computing standards. This section highlights
tooling strategies to support metadata indexing, driver enhancements,
and diagnostics, applicable to Python and other languages.

\begin{itemize}
\tightlist
\item
  \textbf{Enhanced HDF5 Drivers:} Improve drivers like \texttt{h5py} and
  \texttt{ROS3} to better handle cloud object storage's nuances, such as
  intelligent request batching and speculative reads. This mitigates
  inefficiencies caused by high-latency networks.
\item
  \textbf{Metadata Indexing:} Develop tools for pre-indexing metadata,
  similar to Kerchunk. These tools should enable clients to retrieve
  only necessary data offsets, avoiding full metadata reads and
  improving access times.
\item
  \textbf{Kerchunk-like Integration:} Extend Kerchunk to integrate
  seamlessly with analysis libraries like Xarray. This includes building
  robust sidecar files that efficiently map hierarchical datasets,
  enabling faster partial reads and enhancing cloud-native workflows.
\item
  \textbf{Diagnostic Tools:} Create tools for diagnostics and
  performance profiling tailored to cloud-optimized HDF5 files. These
  tools should identify bottlenecks in access patterns and recommend
  adjustments in configurations or chunking strategies.
\end{itemize}

\subsection{Mission implementation}\label{mission-implementation}

ATL03 is a complex science data product containing both segmented (20
meters along-track) and large, variable-rate photon datasets. ATL03 is
created using pipeline-style processing where the science data and
NetCDF-style metadata are written by independent software packages. The
following steps were employed to create cloud-optimized Release 007
ATL03 products, while minimizing increases in file size:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set the ``file space strategy'' to H5F\_FSPACE\_STRATEGY\_PAGE and
  enabled ``free space tracking'' within the HDF5 file creation property
  list.
\item
  Set the ``file space page size'' to 8MiB.
\item
  Change all ``COMPACT'' dataset storage types to ``CONTIGUOUS''.
\item
  Increase the ``chunk size'' of the photon-rate datasets (from 10,000
  to 100,000 elements), while making sure no ``chunk sizes'' exceed the
  8MiB ``file space page size''.
\item
  Introduce a new production step that executes the ``h5repack'' utility
  (with no options) to create a ``defragmented'' final product.
\end{enumerate}

\subsection{Discussion and Further
Work}\label{discussion-and-further-work}

We believe that implementing cloud optimized HDF5 will greatly improve
downstream workflows that will unlock science in the cloud. We also
recognize that in order to get there, some key factors in the ecosystem
need to be addressed. Chunking strategies, adaptive caching and
automatic driver configurations should be developed to optimize
performance.

Efforts should expand multi-language support, creating universal
interfaces and libraries for broader adoption beyond Python.
Cloud-native enhancements must focus on optimizing HDF5 for distributed
systems and object storage, addressing egress costs, ease of use and
scalability. Finally, advancing ecosystem interoperability involves
setting integration standards and aligning with emerging trends such as
serverless and edge computing. These efforts, combined with community
collaboration, will modernize HDF5 to meet the challenges of evolving
data-intensive applications.

\subsubsection{Chunking Shapes and
Sizes}\label{chunking-shapes-and-sizes}

Optimizing chunk shapes and sizes is essential for efficient HDF5 usage,
especially in cloud environments:

\begin{itemize}
\tightlist
\item
  \textbf{Chunk Shape:} Align chunk dimensions with anticipated access
  patterns. For example, row-oriented queries benefit from row-aligned
  chunks.
\item
  \textbf{Chunk Size:} Use chunk sizes between 1--10 MB to match cloud
  storage block sizes. Larger chunks improve sequential access but
  require more memory. Smaller chunks support granular reads but may
  increase network overhead.
\end{itemize}

Finally, we recognize that this study has not been as extensive as it
could have been (cross language, multiple datasets) and yet we think we
ran into the key scenarios data producers will face when they start
producing cloud optimized HDf5 files. We think that there is room for
improvement and experimentation with various configurations based on
real-world scenarios is crucial to determine the best performance.

\section{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\vspace{1em}

\bibitem[\citeproctext]{ref-Hoyer2017-su}
Hoyer, S., \& Hamman, J. (2017). Xarray: {N-D} labeled arrays and
datasets in python. \emph{J. Open Res. Softw.}, \emph{5}(1), 10.

\bibitem[\citeproctext]{ref-h5cloud2023}
ICESAT-2 HackWeek, H. C. (2023). h5cloud: Tools for cloud-based analysis
of HDF5 data (Version v1.0.0). Retrieved from
\url{https://github.com/ICESAT-2HackWeek/h5cloud}

\bibitem[\citeproctext]{ref-Mozilla-latency-2024}
MDN, M. (2024, May). Understanding latency. Retrieved from
\url{https://developer.mozilla.org/en-US/docs/Web/Performance/Understanding_latency}

\bibitem[\citeproctext]{ref-NEUMANN2019111325}
Neumann, T. A., Martino, A. J., Markus, T., Bae, S., Bock, M. R.,
Brenner, A. C., et al. (2019). The ice, cloud, and land elevation
satellite -- 2 mission: A global geolocated photon product derived from
the advanced topographic laser altimeter system. \emph{Remote Sensing of
Environment}, \emph{233}, 111325.
https://doi.org/\url{https://doi.org/10.1016/j.rse.2019.111325}

\bibitem[\citeproctext]{ref-scott-2020}
Scott, C. (2020). Numbers every programmer should know. Retrieved from
\url{https://colin-scott.github.io/personal_website/research/interactive_latency.html}

\bibitem[\citeproctext]{ref-The_HDF_Group_Hierarchical_Data_Format}
The HDF Group. (n.d.). {Hierarchical Data Format, version 5}. Retrieved
from \url{https://github.com/HDFGroup/hdf5}

\end{CSLReferences}




\end{document}
